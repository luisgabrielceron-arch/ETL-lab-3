{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a0a4c03",
   "metadata": {},
   "source": [
    "# ETL Data Warehouse Tutorial\n",
    "## Building a Complete BI System for Retail Analytics\n",
    "\n",
    "This notebook teaches you how to:\n",
    "1. **Extract** raw data from CSV files\n",
    "2. **Transform** data into a dimensional model\n",
    "3. **Load** data into a Data Warehouse\n",
    "4. **Query** the data warehouse for business insights\n",
    "5. **Calculate** KPIs for decision making\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b27c65b",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Dimensional Modeling\n",
    "\n",
    "### OLTP vs OLAP\n",
    "- **OLTP** (Online Transaction Processing): Normalized databases for fast transactions (your RAW data)\n",
    "- **OLAP** (Online Analytical Processing): Denormalized data warehouse for fast queries (your DW)\n",
    "\n",
    "### Star Schema\n",
    "A star schema consists of:\n",
    "- **Fact Table**: Contains metrics (measures) and foreign keys to dimensions\n",
    "  - Example: Sales Fact Table with quantity, revenue, cost, profit\n",
    "- **Dimension Tables**: Contain descriptive attributes\n",
    "  - Product Dimension (category, brand, price)\n",
    "  - Customer Dimension (city, country, age)\n",
    "  - Channel Dimension (store type)\n",
    "  - Date Dimension (day, month, quarter, year)\n",
    "\n",
    "### Example: Star Schema for Retail\n",
    "```\n",
    "                    DIM_DATE\n",
    "                      |\n",
    "        DIM_PRODUCT - FACT_SALES - DIM_CUSTOMER\n",
    "                      |\n",
    "                   DIM_CHANNEL\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb647d1",
   "metadata": {},
   "source": [
    "## Part 2: Setup - Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07194f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up paths\n",
    "project_root = r'c:\\Users\\LENOVO\\OneDrive\\Documentos\\ETL\\3 Lab\\ETL-lab-3'\n",
    "raw_data_path = os.path.join(project_root, 'data', 'raw')\n",
    "warehouse_path = os.path.join(project_root, 'data', 'warehouse')\n",
    "etl_path = os.path.join(project_root, 'ETL')\n",
    "db_path = os.path.join(warehouse_path, 'datawarehouse.db')\n",
    "\n",
    "# Create warehouse folder if it doesn't exist\n",
    "os.makedirs(warehouse_path, exist_ok=True)\n",
    "\n",
    "# Add ETL folder to Python path\n",
    "sys.path.insert(0, etl_path)\n",
    "\n",
    "print(\"✓ Environment configured\")\n",
    "print(f\"  Raw data path: {raw_data_path}\")\n",
    "print(f\"  Database path: {db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e4276e",
   "metadata": {},
   "source": [
    "## Part 3: EXTRACT Phase\n",
    "\n",
    "The Extract phase reads raw data from CSV files and validates the schema.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Read files in a structured way\n",
    "- Validate that all expected columns exist\n",
    "- Check for empty tables\n",
    "- Convert data types appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ddbd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the extract module\n",
    "from extract import DataExtractor\n",
    "\n",
    "# Initialize the extractor\n",
    "extractor = DataExtractor(raw_data_path)\n",
    "\n",
    "# Extract all data\n",
    "extracted_data = extractor.extract_all()\n",
    "\n",
    "# Show what was extracted\n",
    "print(\"\\nExtracted Data Summary:\")\n",
    "for table_name, df in extracted_data.items():\n",
    "    print(f\"\\n{table_name.upper()}:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the raw data - Products\n",
    "print(\"Products Table:\")\n",
    "print(extracted_data['products'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a47c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSales Table:\")\n",
    "print(extracted_data['sales'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277caf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCustomers Table:\")\n",
    "print(extracted_data['customers'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80af970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nChannels Table:\")\n",
    "print(extracted_data['channels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127e224b",
   "metadata": {},
   "source": [
    "## Part 4: TRANSFORM Phase\n",
    "\n",
    "The Transform phase prepares data for the data warehouse.\n",
    "\n",
    "**What happens here:**\n",
    "1. **Create Date Dimension** - Useful for time-based analysis (trends, seasonality)\n",
    "2. **Create Product Dimension** - Denormalized product data with calculations\n",
    "3. **Create Customer Dimension** - Customer attributes\n",
    "4. **Create Channel Dimension** - Sales channel information\n",
    "5. **Create Fact Table** - Transactions with measures (quantity, revenue, profit)\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Surrogate Keys**: Sequential integer IDs (better for DW than business keys)\n",
    "- **Measures**: Numeric values that can be aggregated (sum, avg)\n",
    "- **Attributes**: Descriptive fields used for filtering and grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the transform module\n",
    "from transform import DataTransformer\n",
    "\n",
    "# Initialize the transformer\n",
    "transformer = DataTransformer(extracted_data)\n",
    "\n",
    "# Transform all data\n",
    "transformed_data = transformer.transform_all()\n",
    "\n",
    "print(\"\\nTransformed Data Summary:\")\n",
    "for table_name, df in transformed_data.items():\n",
    "    print(f\"\\n{table_name.upper()}:\")\n",
    "    print(f\"  Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad81b637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the Date Dimension\n",
    "print(\"DATE DIMENSION (First few rows):\")\n",
    "print(transformed_data['dim_date'].head(10))\n",
    "print(f\"\\nDate range: {transformed_data['dim_date']['date'].min()} to {transformed_data['dim_date']['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82d34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine Product Dimension with calculated margins\n",
    "print(\"PRODUCT DIMENSION (with calculated margins):\")\n",
    "product_check = transformed_data['dim_product'][['product_key', 'product_id', 'name', 'category', 'brand', 'unit_price', 'unit_cost', 'margin_percent']]\n",
    "print(product_check.head(10))\n",
    "print(f\"\\nTotal products: {len(transformed_data['dim_product'])}\")\n",
    "print(f\"Categories: {transformed_data['dim_product']['category'].unique()}\")\n",
    "print(f\"Brands: {transformed_data['dim_product']['brand'].nunique()} brands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6676a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine Sales Fact Table\n",
    "print(\"SALES FACT TABLE (with calculated measures):\")\n",
    "print(transformed_data['fact_sales'].head())\n",
    "print(f\"\\nFact table shape: {transformed_data['fact_sales'].shape}\")\n",
    "print(f\"Total sales amount: ${transformed_data['fact_sales']['total_sales_amount'].sum():,.2f}\")\n",
    "print(f\"Total profit: ${transformed_data['fact_sales']['profit'].sum():,.2f}\")\n",
    "print(f\"Average profit margin: {transformed_data['fact_sales']['profit_margin'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad47fd",
   "metadata": {},
   "source": [
    "## Part 5: LOAD Phase\n",
    "\n",
    "The Load phase creates the Data Warehouse database and loads the dimensions and fact table.\n",
    "\n",
    "**Loading Order:**\n",
    "1. Create the schema (all tables)\n",
    "2. Load dimension tables FIRST (no dependencies)\n",
    "3. Load fact table LAST (uses foreign keys to dimensions)\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Primary Key**: Unique identifier for each row\n",
    "- **Foreign Key**: Reference to another table's primary key\n",
    "- **Referential Integrity**: Ensures all foreign key references point to valid records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5bcbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the load module\n",
    "from load import DataWarehouseLoader\n",
    "\n",
    "# Initialize the loader\n",
    "loader = DataWarehouseLoader(db_path)\n",
    "\n",
    "# Load all data into the warehouse\n",
    "loader.load_all(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3103d799",
   "metadata": {},
   "source": [
    "## Part 6: Query the Data Warehouse\n",
    "\n",
    "Now we can query the warehouse to get business insights. The dimensional model makes these queries fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f343f4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the data warehouse\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Query 1: Total sales and profit by product category\n",
    "query1 = \"\"\"\n",
    "SELECT \n",
    "    p.category,\n",
    "    COUNT(f.sales_key) as num_transactions,\n",
    "    SUM(f.quantity) as total_quantity,\n",
    "    ROUND(SUM(f.total_sales_amount), 2) as total_revenue,\n",
    "    ROUND(SUM(f.profit), 2) as total_profit,\n",
    "    ROUND(AVG(f.profit_margin), 2) as avg_profit_margin\n",
    "FROM fact_sales f\n",
    "JOIN dim_product p ON f.product_key = p.product_key\n",
    "GROUP BY p.category\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "kpi1 = pd.read_sql_query(query1, conn)\n",
    "print(\"KPI 1: Sales Volume and Revenue by Product Category\")\n",
    "print(kpi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: Revenue by channel (Physical Store vs Online)\n",
    "query2 = \"\"\"\n",
    "SELECT \n",
    "    c.channel,\n",
    "    COUNT(f.sales_key) as num_transactions,\n",
    "    SUM(f.quantity) as total_quantity,\n",
    "    ROUND(SUM(f.total_sales_amount), 2) as total_revenue,\n",
    "    ROUND(SUM(f.profit), 2) as total_profit,\n",
    "    ROUND(100 * SUM(f.total_sales_amount) / (SELECT SUM(total_sales_amount) FROM fact_sales), 2) as revenue_pct\n",
    "FROM fact_sales f\n",
    "JOIN dim_channel c ON f.channel_key = c.channel_key\n",
    "GROUP BY c.channel\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "kpi2 = pd.read_sql_query(query2, conn)\n",
    "print(\"\\nKPI 2: Revenue by Sales Channel\")\n",
    "print(kpi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722571f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: Monthly sales trends\n",
    "query3 = \"\"\"\n",
    "SELECT \n",
    "    d.year,\n",
    "    d.month,\n",
    "    d.month_name,\n",
    "    COUNT(f.sales_key) as num_transactions,\n",
    "    ROUND(SUM(f.total_sales_amount), 2) as monthly_revenue,\n",
    "    ROUND(SUM(f.profit), 2) as monthly_profit,\n",
    "    ROUND(AVG(f.total_sales_amount), 2) as avg_transaction_value\n",
    "FROM fact_sales f\n",
    "JOIN dim_date d ON f.date_id = d.date_id\n",
    "GROUP BY d.year, d.month, d.month_name\n",
    "ORDER BY d.year, d.month\n",
    "\"\"\"\n",
    "\n",
    "kpi3 = pd.read_sql_query(query3, conn)\n",
    "print(\"\\nKPI 3: Monthly Sales Trends\")\n",
    "print(kpi3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11732c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 4: Top brands by revenue\n",
    "query4 = \"\"\"\n",
    "SELECT \n",
    "    p.brand,\n",
    "    COUNT(f.sales_key) as num_transactions,\n",
    "    SUM(f.quantity) as total_units_sold,\n",
    "    ROUND(SUM(f.total_sales_amount), 2) as brand_revenue,\n",
    "    ROUND(SUM(f.profit), 2) as brand_profit,\n",
    "    ROUND(AVG(p.margin_percent), 2) as avg_margin_percent\n",
    "FROM fact_sales f\n",
    "JOIN dim_product p ON f.product_key = p.product_key\n",
    "GROUP BY p.brand\n",
    "ORDER BY brand_revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "kpi4 = pd.read_sql_query(query4, conn)\n",
    "print(\"\\nKPI 4: Performance by Brand\")\n",
    "print(kpi4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5ea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 5: Customer geography analysis\n",
    "query5 = \"\"\"\n",
    "SELECT \n",
    "    c.country,\n",
    "    COUNT(DISTINCT c.customer_key) as num_customers,\n",
    "    COUNT(f.sales_key) as total_transactions,\n",
    "    ROUND(SUM(f.total_sales_amount), 2) as country_revenue,\n",
    "    ROUND(AVG(f.total_sales_amount), 2) as avg_transaction_value\n",
    "FROM fact_sales f\n",
    "JOIN dim_customer c ON f.customer_key = c.customer_key\n",
    "GROUP BY c.country\n",
    "ORDER BY country_revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "kpi5 = pd.read_sql_query(query5, conn)\n",
    "print(\"\\nKPI 5: Sales by Customer Geography\")\n",
    "print(kpi5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa3d8aa",
   "metadata": {},
   "source": [
    "## Part 7: Data Warehouse Visualizations\n",
    "\n",
    "Now let's create visual representations of our KPIs for business presentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44988a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Data Warehouse KPI Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Revenue by Category (Bar Chart)\n",
    "ax1 = axes[0, 0]\n",
    "kpi1_sorted = kpi1.sort_values('total_revenue', ascending=True)\n",
    "ax1.barh(kpi1_sorted['category'], kpi1_sorted['total_revenue'], color='steelblue')\n",
    "ax1.set_xlabel('Revenue ($)', fontweight='bold')\n",
    "ax1.set_title('Revenue by Product Category', fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "for i, v in enumerate(kpi1_sorted['total_revenue']):\n",
    "    ax1.text(v + 5000, i, f'${v:,.0f}', va='center')\n",
    "\n",
    "# Plot 2: Revenue by Channel (Pie Chart)\n",
    "ax2 = axes[0, 1]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "ax2.pie(kpi2['total_revenue'], labels=kpi2['channel'], autopct='%1.1f%%', \n",
    "        colors=colors, startangle=90, textprops={'fontsize': 10})\n",
    "ax2.set_title('Revenue Distribution by Channel', fontweight='bold')\n",
    "\n",
    "# Plot 3: Monthly Revenue Trend (Line Chart)\n",
    "ax3 = axes[1, 0]\n",
    "kpi3_sorted = kpi3.sort_values('month')\n",
    "ax3.plot(kpi3_sorted['month_name'], kpi3_sorted['monthly_revenue'], \n",
    "         marker='o', linewidth=2, markersize=8, color='darkgreen')\n",
    "ax3.fill_between(range(len(kpi3_sorted)), kpi3_sorted['monthly_revenue'], alpha=0.3, color='green')\n",
    "ax3.set_xlabel('Month', fontweight='bold')\n",
    "ax3.set_ylabel('Revenue ($)', fontweight='bold')\n",
    "ax3.set_title('Monthly Revenue Trend', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 4: Top Brands by Revenue (Horizontal Bar)\n",
    "ax4 = axes[1, 1]\n",
    "kpi4_top = kpi4.head(8).sort_values('brand_revenue', ascending=True)\n",
    "ax4.barh(kpi4_top['brand'], kpi4_top['brand_revenue'], color='coral')\n",
    "ax4.set_xlabel('Revenue ($)', fontweight='bold')\n",
    "ax4.set_title('Top 8 Brands by Revenue', fontweight='bold')\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "for i, v in enumerate(kpi4_top['brand_revenue']):\n",
    "    ax4.text(v + 5000, i, f'${v:,.0f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Dashboard created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e509502",
   "metadata": {},
   "source": [
    "## Part 8: Advanced Analysis - Profitability Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a147048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query: Product profitability analysis\n",
    "query_products = \"\"\"\n",
    "SELECT \n",
    "    p.product_id,\n",
    "    p.name,\n",
    "    p.brand,\n",
    "    p.category,\n",
    "    COUNT(f.sales_key) as times_sold,\n",
    "    ROUND(SUM(f.total_sales_amount), 2) as product_revenue,\n",
    "    ROUND(SUM(f.profit), 2) as product_profit,\n",
    "    ROUND(AVG(f.profit_margin), 2) as profit_margin,\n",
    "    ROUND(SUM(f.total_sales_amount) - SUM(f.total_cost), 2) as total_profit\n",
    "FROM fact_sales f\n",
    "JOIN dim_product p ON f.product_key = p.product_key\n",
    "GROUP BY p.product_id, p.name, p.brand, p.category\n",
    "HAVING times_sold > 0\n",
    "ORDER BY product_profit DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "top_products = pd.read_sql_query(query_products, conn)\n",
    "print(\"Top 10 Most Profitable Products:\")\n",
    "print(top_products.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query: Best and worst performing products\n",
    "query_worst = \"\"\"\n",
    "SELECT \n",
    "    p.name,\n",
    "    p.brand,\n",
    "    COUNT(f.sales_key) as times_sold,\n",
    "    ROUND(SUM(f.total_sales_amount), 2) as revenue,\n",
    "    ROUND(SUM(f.profit), 2) as profit\n",
    "FROM fact_sales f\n",
    "JOIN dim_product p ON f.product_key = p.product_key\n",
    "GROUP BY p.product_id, p.name, p.brand\n",
    "HAVING times_sold > 0\n",
    "ORDER BY profit ASC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "worst_products = pd.read_sql_query(query_worst, conn)\n",
    "print(\"\\nWorst 5 Performing Products (by profit):\")\n",
    "print(worst_products.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26c1995",
   "metadata": {},
   "source": [
    "## Part 9: Business Summary & Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088324ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key metrics\n",
    "summary_query = \"\"\"\n",
    "SELECT \n",
    "    COUNT(DISTINCT f.sales_key) as total_transactions,\n",
    "    COUNT(DISTINCT f.customer_key) as unique_customers,\n",
    "    COUNT(DISTINCT f.product_key) as products_sold,\n",
    "    COUNT(DISTINCT f.date_id) as days_active,\n",
    "    COUNT(DISTINCT f.channel_key) as channels,\n",
    "    SUM(f.quantity) as total_units,\n",
    "    ROUND(SUM(f.total_sales_amount), 2) as total_revenue,\n",
    "    ROUND(SUM(f.total_cost), 2) as total_cost,\n",
    "    ROUND(SUM(f.profit), 2) as total_profit,\n",
    "    ROUND(100 * SUM(f.profit) / SUM(f.total_sales_amount), 2) as profit_margin_percent\n",
    "FROM fact_sales f\n",
    "\"\"\"\n",
    "\n",
    "summary = pd.read_sql_query(summary_query, conn)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BUSINESS SUMMARY - KEY METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Transactions: {summary['total_transactions'].values[0]:,}\")\n",
    "print(f\"Unique Customers: {summary['unique_customers'].values[0]:,}\")\n",
    "print(f\"Products Sold: {summary['products_sold'].values[0]:,}\")\n",
    "print(f\"Active Days: {summary['days_active'].values[0]:,}\")\n",
    "print(f\"Sales Channels: {summary['channels'].values[0]:,}\")\n",
    "print(f\"Total Units Sold: {summary['total_units'].values[0]:,}\")\n",
    "print(f\"\\nTotal Revenue: ${summary['total_revenue'].values[0]:,.2f}\")\n",
    "print(f\"Total Cost: ${summary['total_cost'].values[0]:,.2f}\")\n",
    "print(f\"Total Profit: ${summary['total_profit'].values[0]:,.2f}\")\n",
    "print(f\"Overall Profit Margin: {summary['profit_margin_percent'].values[0]:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2257ea",
   "metadata": {},
   "source": [
    "## Part 10: Running the Complete ETL Pipeline\n",
    "\n",
    "Here's how to run the entire ETL process end-to-end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c8b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main ETL Pipeline\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPLETE ETL PIPELINE EXECUTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# STEP 1: EXTRACT\n",
    "print(\"\\nSTEP 1: EXTRACT\")\n",
    "print(\"-\" * 60)\n",
    "from extract import extract\n",
    "extracted = extract(raw_data_path)\n",
    "\n",
    "# STEP 2: TRANSFORM\n",
    "print(\"\\nSTEP 2: TRANSFORM\")\n",
    "print(\"-\" * 60)\n",
    "from transform import transform\n",
    "transformed = transform(extracted)\n",
    "\n",
    "# STEP 3: LOAD\n",
    "print(\"\\nSTEP 3: LOAD\")\n",
    "print(\"-\" * 60)\n",
    "from load import load\n",
    "load(transformed, db_path)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ ETL PIPELINE COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nYour Data Warehouse is ready for analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6030b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close database connection\n",
    "conn.close()\n",
    "print(\"\\n✓ Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b1d150",
   "metadata": {},
   "source": [
    "## Learning Summary\n",
    "\n",
    "### What You Learned:\n",
    "1. **Dimensional Modeling** - Designing star schemas for analytics\n",
    "2. **ETL Concepts** - Extract, Transform, Load stages\n",
    "3. **Data Quality** - Schema validation and referential integrity\n",
    "4. **Surrogate Keys** - Why and how to use them\n",
    "5. **KPI Calculation** - Computing business intelligence metrics\n",
    "6. **Visualization** - Creating dashboards from dimensional data\n",
    "\n",
    "### Key Files Created:\n",
    "- `extract.py` - Reads and validates raw data\n",
    "- `transform.py` - Creates dimensions and fact tables\n",
    "- `load.py` - Loads data into SQLite data warehouse\n",
    "- `datawarehouse.db` - Your complete data warehouse\n",
    "\n",
    "### Next Steps:\n",
    "- Modify queries to answer specific business questions\n",
    "- Add more dimensions (e.g., product reviews, customer segments)\n",
    "- Create additional fact tables for different business processes\n",
    "- Build dashboards in Power BI or Tableau\n",
    "- Automate the ETL pipeline to run on schedules\n",
    "\n",
    "### Data Warehouse Benefits:\n",
    "✓ Fast queries for business analysis\n",
    "✓ Centralized source of truth\n",
    "✓ Easy to understand structure\n",
    "✓ Supports complex reporting needs\n",
    "✓ Foundation for ML/AI projects"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
